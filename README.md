# fine-tuning_of_LLMs_with_hugging_face
Short Jypter notebook tutorial on tuning an LLM using the Hugging Face library.

This project demonstrates how to fine-tune large language models (LLMs) using the Hugging Face library. The code fine-tunes a pre-trained LLaMA model on a dataset of medical terms and provides an example of generating text with the fine-tuned model.

## Key Features
- Fine-tuning a large language model with Hugging Face.
- Using PEFT (Parameter Efficient Fine-Tuning) with LoRA (Low-Rank Adaptation).
- Supervised fine-tuning with specific training arguments.
- Example of text generation using the fine-tuned model.

## Contributing
Contributions are welcome! Please open an issue or submit a pull request.

## License
This project is licensed under the MIT License.
